#!/bin/bash -eu


################################################################################
# 
#   'bred': 'bashreduce', mapreduce in bash (by erik@fawx.com), enhanced
#
#                                                      dakusui@gmail.com
#
#   Feb/26/2015 Changes from 'bashreduce':
#   * Abolished 'brm': I gave up making it consistent with 'sort -m'.
#   * Created 'map' and 'reduce' behaviour modes, which allow you to perform
#     tasks more normal mapreduce way. The behavior erikfrey created is now 
#     called 'compat' and it is a default.
#   * Made it possible to specify job_id (-j option) by an integer. Based on this
#     number bred allocates ports to be used and now you can use bred multiple 
#     times in a command line connecting them pipes.
#   * Made it possible to specify base port.
#   * Added 'sort_opt' option to be able to make sort commands use it. This is
#     useful e.g., when you want to sort the output by a numeric field. Use "-s '-n'"
#   * Reorganized internal structure.
# 
################################################################################

usage() {
  local prog="`basename $1`"
  echo "Usage: $prog [-m host1 [host2...]] [-c column] [-r userfunc] [-i input] [-o output] [-e error]"
  echo "       $prog -h for help."
  exit 2
}

showhelp() {
  echo "Usage: `basename $1`: [-m host1 [host2...]] [-c column] [-r userfunc] [-i input] [-o output] [-e error]"
  echo   "bred. bashreduce enhanced.  Map an input file to many hosts, sort/reduce, merge"
  echo   "  -m: hosts to use, can repeat hosts for multiple cores"
  echo   "      default hosts from ~/.br.hosts or /etc/br.hosts"
  echo   "  -p: base port, default = 8192"
  echo   "  -M: mode ('map', 'reduce', 'compat'), default = 'compat'"
  echo   "  -I: interpreter. ignored if 'compat' mode is used, default = 'bash -c'"
  echo   "  -j: job index, default = 0"
  echo   "  -c: column to partition, default = 1 (1-based)"
  echo   "  -i: input file, default = stdin"
  echo   "  -o: output file, default = stdout"
  echo   "  -e: error file, default = stderr"
  echo   "  -r: userfunc function, default = identity"
  echo   "  -t: tmp dir to use, default = $HOME/.bred"
  echo   "  -S: memory to use for 'S'ort, default = 256M"
  echo   "  -s: column to 's'ort(s) on reduce (and following merge). , default is equal to -c "
  echo   "  -T: option given to sor'T'(s) on reduce (and following merge), default is nothing"
  echo   "  -O: perform sort on output, ignored in 'compat' mode., default = yes"
  echo   "  -v: verbose mode. default = no"
  echo   "  -h: this help message"
  exit 2
} 


# int
#   
# out
#   jobid    "${OPTARG:-$(uuid)}"
#   jobpath  "${tmp_dir:-$HOME/.bred}/jm/${job_idx}/job"
#   nodepath "${tmp_dir:-$HOME/.bred}/jm/${job_idx}/nodes/${host_idx}"
# local
bred_parseopt() {
  LC_ALL=${LC_ALL:-""}
  base_port=8192
  job_idx=0
  mode='compat'
  interpreter='bash -c'
  hosts=
  mapcolumn=1
  userfunc=
  input=
  output=
  error=
  tmp_dir=${HOME}/.bred
  sort_mem=256M
  sort_column=
  sort_on_out="yes"
  sort_opt=
  verbose="no"
  
  while getopts "m:p:j:M:I:c:r:i:o:e:t:S:s:T:O:v:h" name; do
    case "$name" in
      m)  hosts=$OPTARG;;
      p)  baseport=$OPTARG;;
      M)  mode=$OPTARG;;
      I)  interpreter=$OPTARG;;
      c)  mapcolumn=$OPTARG;;
      r)  userfunc=$OPTARG;;
      i)  input=$OPTARG;;
      o)  output=$OPTARG;;
      e)  error=$OPTARG;;
      t)  tmp_dir=$OPTARG;;
      S)  sort_mem=$OPTARG;;
      s)  sort_column=$OPTARG;;
      O)  sort_on_out=$OPTARG;;
      T)  sort_opt=$OPTARG;;
      v)  verbose="yes";;
      h)  showhelp $0;;
      [?])  usage $0;;
    esac
  done
  # job_idx
  local _nextid=-1
  local _jid_dir="${tmp_dir}/jm"
  job_idx="$(_nextid=$(echo $(($(ls ${_jid_dir}| sort -n -r  | head --lines=1) + 1))) \
           && mkdir -p ${_jid_dir}/${_nextid} && echo ${_nextid} || echo -1)"
  if [[ "${job_idx}" == "" ]]; then
    echo "${job_idx:?Failed to get job id. : ${_jid_dir}}" 1>&2
  fi
  echo "retrieved job id: ${job_idx:?Failed to get job id. }: ${_jid_dir}" 1>&2
  echo "$$" > "${_jid_dir}/${job_idx}/pid"

  hosts=$(bred_load_brhosts "$hosts")
  sort_column=${sort_column:-$mapcolumn}
  # check if 'hosts' is set.
  hosts=${hosts:?"`basename $0`: must specify hosts with -m or provide ~/.br.hosts or /etc/br.hosts"}
  case "$mode" in
    compat|map|reduce)
      ;;
    *)
      echo "`basename $0`: unknown mode is specified. ($mode)"
      usage $0
      ;;
  esac
  job_id=${jobid:-"`uuidgen`"}
}

# * in
#   (params)     mode, host_idx, nodepath
#   (enviromnet) LC_ALL
#   (from opt)   mapcolumn, interpreter, userfunc, sort_mem, tmp_dir
# * out
#   task
# * local
#   sorter, usercode, envvars
bred_compose_task() {
    # mode: 'map', 'reduce', 'compat'
    local mode=$1
    local host_idx=$2
    local usercode_errlog="${3}/${host_idx}/err-usercode.log"
    local sorter_errolog="${3}/${host_idx}/err-sorter.log"

    sorter="2>$sorter_errolog"
    if [ "$sort_on_out" == "yes" ] ; then
	sorter="| LC_ALL='$LC_ALL' sort -S$sort_mem -T$tmp_dir -k$sort_column,$sort_column $sort_opt  2>$sorter_errolog"
    fi

    local usercode=
    local envvars=
    local _namenode=$(hostname)
    local _hosts="${hosts}"
    local _bredfs_init=$(bredfs_compose_init "${tmp_dir}/fs" | base64 -w 0)
    local _bredfs_write=$(bredfs_compose_write "${_namenode}" '${tmp_dir}/fs' | base64 -w 0)
    local _bredfs_read=$(bredfs_compose_read "${tmp_dir}/fs" "${host_idx:-'-'}" "${_hosts}" | base64 -w 0)
    envvars="BRED_PART_ID=\"$host_idx\" BRED_KEYINDEX=\"$mapcolumn\" BRED_INTERPRETER=\"$interpreter\""
    envvars="${envvars} BRED_USERCODE='$userfunc'"
    envvars="${envvars} BREDFS_INIT='${_bredfs_init}'"
    envvars="${envvars} BREDFS_WRITE='${_bredfs_write}'"
    envvars="${envvars} BREDFS_READ='${_bredfs_read}'"

    case "$mode" in
        map)
            if [[ -z "$userfunc" ]]; then
                echo "`basename $0`: $mode is specified. must specify userfunc with -r"
                usage $1
            fi
            usercode='{
                cmd=sprintf("%s %c%s%c", ENVIRON["BRED_INTERPRETER"], 39, ENVIRON["BRED_USERCODE"], 39);
                print | cmd; 
            }'
            task="| $envvars $AWK '$usercode' $sorter"
            ;;
        reduce)
            if [[ -z "$userfunc" ]]; then
                echo "`basename $0`: ${mode} is specified. must specify userfunc with -r"
                usage $1
            fi
            case ${interpreter} in
                "awk-native")
                    usercode='BEGIN {
                        bred_firsttime=1;
                        bred_key=FS;
                        bred_keyindex=ENVIRON["BRED_KEYINDEX"];
                    }
                    {
                        if (bred_key!=$bred_keyindex) {
                          if (bred_firsttime!=1) {
                              bredEndReduce();
                          }
                          bredBeginReduce(bred_keyindex);
                        }
                        bredReduce(bred_keyindex);
                        bred_firsttime=0;
                        bred_key=$bred_keyindex;
                    }
                    END {
                        if (bred_firsttime==0) {
                            bredEndReduce();
                        }
                    }
                    __USERCODE__'
                    #
                    #  __USERCODE__ is replaced with a string provided by user using '-r' option.
                    #  Below is an example to perform 'word count'.
                    #
                    #  function bredBeginReduce(key_idx) {
                    #    i=0;
                    #    key=$key_idx;
                    #    print "BEGIN", key, "(", key_idx, ")"
                    #  }
                    #  function bredReduce(key_idx) {
                    #     print "REDUCE", $key_idx
                    #     i++;
                    #  }
                    #  function bredEndReduce() {
                    #    print "END", key, i;
                    #  }
                    usercode=${usercode//__USERCODE__/${userfunc}}
                    task="| $envvars $AWK '$usercode' 2> $usercode_errlog $sorter"
                    ;;
                *)
                    usercode='BEGIN {
                        cmd=sprintf("%s %c%s%c", ENVIRON["BRED_INTERPRETER"], 39, ENVIRON["BRED_USERCODE"], 39);
                        firsttime=1;
                        key="";
                        keyindex=ENVIRON["BRED_KEYINDEX"];
                    }
                    {
                        if (key!=$keyindex) {
                          if (firsttime!=1) {
                              close(cmd);
                          }
                        }
                        print | cmd;
                        firsttime=0;
                        key=$keyindex;
                    }
                    END {
                        if (firsttime==0) {
                            close(cmd);
                        }
                    }'
                    task="| $envvars $AWK '$usercode' 2> $usercode_errlog $sorter"
                    ;;
            esac
            ;;
        compat)
            if [ -n "$userfunc" ]; then
                # if we have a userfunc, add the pipe explicitly
                usercode="| $envvars $interpreter '$userfunc' 2> $usercode_errlog"
            else
                usercode=""
            fi
            task="$sorter $usercode"
            ;;
        *)
            echo "Unknown mode '$1' was given."
            exit 1
            ;;
    esac
}

# Executes a map/reduce/compat task
# 
# variables:
# * read
#   (from opt) hosts, mode, mapcolumn, userfunc, solrt_column
#   (globals)  sorter, tmp_dir
# * write
#   (none)
# * local
#   host, pid, envvars, port_in, port_out, host_idx, out_files, in_filed, input, num_hosts
bred_executetask() {
  AWK="/usr/bin/awk"
  # okay let's get started!  first we need a name for our job
  local jobpath="$tmp_dir/jm/${job_idx}/job"
  local nodepath="$tmp_dir/jm/${job_idx}/nodes"
  local num_hosts=$(($(echo "$hosts" | wc -w)))
  local port_in=$(($base_port + $num_hosts * $job_idx * 2))
  local port_out=$(($port_in + 1))
  local host_idx=0
  local out_files=
  local in_files=
  
  # now, for each host, set up in and out fifos (and a netcat for each), and ssh to each host to set up workers listening on netcat
  mkdir -p "${jobpath}/"{in,out}
  for host in ${hosts}; do
    ####
    # set up communication path
    # 1) master side
    #    our named pipes
    mkfifo "${jobpath}/"{in,out}"/${host_idx}"
    # 2) slave side
    ssh -n "$host" "mkdir -p ${nodepath}/${host_idx}"
    #    let's start the remote 'nc' listener and get the pid of it
    local pid=$(ssh -n "$host" "nc -l -p $port_out >${nodepath}/${host_idx}/in 2>/dev/null </dev/null & jobs -l" | awk {'print $2'})
    
    ####
    # start task process on remote side and connect to it to the listener
    # 1) compose 'task' component based on execution mode (map, reduce, or compat)
    bred_compose_task "$mode" "$host_idx" "${nodepath}"
    # 2) start the task.
    ssh "$host" -n "\
            tail -s0.1 -f --pid=$pid ${nodepath}/${host_idx}/in 2>/dev/null </dev/null \
                    $task \
                    | nc -q0 -l -p $port_in >&/dev/null &"
    
    ####
    # Connect the task to the local forwarder of master side's
    # our local forwarders
    nc "${host}" "${port_in}" >"${jobpath}/in/${host_idx}" &
    nc -q0 "${host}" "${port_out}" <"${jobpath}/out/${host_idx}" &
    # our vars
    out_files="$out_files $jobpath/out/$host_idx"
    in_files="$in_files $jobpath/in/$host_idx"
    port_in=$(($port_in + 2))
    port_out=$(($port_in + 1))
    host_idx=$(($host_idx + 1))
  done
  
  ####    
  # okay, time to map
  if which brp >/dev/null; then
    eval "${input:+pv $input |} brp - $((${mapcolumn} - 1)) ${out_files}"
  else
    ####
    # use awk if we don't have 'brp'
    # we're taking advantage of a special property that awk leaves its file handles open until its done
    # i think this is universal
    # we're also sending a zero length string to all the handles at the end, in case some pipe got no love
    eval "${input:+pv $input |} awk '{
            srand(\$$mapcolumn);
            print \$0 >>\"$jobpath/out/\"int(rand() * $host_idx);
        }
        END {
            for (i = 0; i != $host_idx; ++i)
            printf \"\" >>\"$jobpath/out/\"i;
        }'"
  fi
  
  ####    
  # save it somewhere
  # use sort -m. 
  # (dakusui: abolished brm to simplify the software since i couldn't convince myself there is a big performance gain)
  # sort -m creates tmp files if too many input files are specified
  # brm doesn't do this
  if [ ${sort_on_out} == "yes" ] ; then
    eval "sort -k${sort_column},${sort_column} -m ${in_files} ${sort_opt} ${output:+| pv >$output}"
  else
    eval "cat ${in_files} ${output:+| pv >$output}"
  fi
  
  ####
  # save errors somewhere
  local i=0;
  for host in ${hosts}; do
    local cmd='for i in $(ls '${nodepath}'/'${i}'/err*); do echo '${i}' $(basename $i); nl -w 1 -s " " $i; echo '${i}' ----; done'
    if [ -n "$error" ]; then
      ssh "$host" "$cmd" >>"$error"
    else
      # By default, redirect stdouts of ssh commands to stderr because the files are originally stderr.
      ssh "$host" "$cmd" 1>&2
    fi
    i=$(($i + 1))
  done
  
  ####    
  # finally, clean up after ourselves
  if [[ -e "jjjj" ]]; then
    rm -rf "${jobpath:?something goes wrong!}"
    for host in ${hosts}; do
      ssh "$host" "rm -fr ${nodepath:?'node path isn't set''}"
    done
    rm -fr "${tmp_dir:?something goes wrong!}/jm/${job_idx:?something goes wrong!}"
  fi
  
  ####    
  # TODO: is there a safe way to kill subprocesses upon fail?
  # this seems to work: /bin/kill -- -$$
}

. "$(dirname $0)/bred-core"
bred_parseopt "$@"
bred_executetask;

